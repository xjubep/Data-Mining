{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Digits dataset 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1437, 64)\n(360, 64)\n(1437,)\n(360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits.data, digits.target, random_state=777, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Digits dataset (참고용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 1797\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 1\n",
    "CART decision Tree classifier.\n",
    "- scikit-learn의 decision tree는 CART 기반 구현체이다.\n",
    "- Problem 1과 같은 방법으로 CART classifier 모델을 학습시키고 test데이터의 정확도를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test data prediction accuracy (CART) : 0.850000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "CART_model = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "CART_model = DecisionTreeClassifier()\n",
    "CART_model.fit(X_train, Y_train)\n",
    "accuracy = CART_model.score(X_test, Y_test)\n",
    "################################################# \n",
    "print('Test data prediction accuracy (CART) : %f'%accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 2\n",
    "Random Forest classifier.\n",
    "- scikit-learn의 random forest 패키지를 이용하여 모델을 학습시키고 test데이터의 정확도를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test data prediction accuracy (Random Forest) : 0.955556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Random_Forest_model = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "Random_Forest_model = RandomForestClassifier()\n",
    "Random_Forest_model.fit(X_train, Y_train)\n",
    "accuracy = Random_Forest_model.score(X_test, Y_test)\n",
    "################################################# \n",
    "print('Test data prediction accuracy (Random Forest) : %f'%accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 3\n",
    "XGBoost classifier.\n",
    "- XGBoost 패키지를 pip install을 통해 설치하시오.\n",
    "- 패키지를 불러와 모델을 학습시키고 test데이터의 정확도를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (1.4.1)\nRequirement already satisfied: scipy in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from xgboost) (1.6.2)\nRequirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from xgboost) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "##########Put your installation code here###############\n",
    "!pip install xgboost\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[19:18:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\user\\.conda\\envs\\DM\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "Test data prediction accuracy (XGBoost) : 0.958333\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "XGB_model = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "XGB_model = xgb.XGBClassifier()\n",
    "XGB_model.fit(X_train, Y_train)\n",
    "accuracy = XGB_model.score(X_test, Y_test)\n",
    "################################################# \n",
    "print('Test data prediction accuracy (XGBoost) : %f'%accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 4\n",
    "LightGBM classifier.\n",
    "- LighGBM 패키지를 pip install을 통해 설치하시오.\n",
    "- 패키지를 불러와 모델을 학습시키고 test데이터의 정확도를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (3.2.1)\nRequirement already satisfied: scipy in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from lightgbm) (1.6.2)\nRequirement already satisfied: wheel in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from lightgbm) (0.36.2)\nRequirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from lightgbm) (1.19.2)\nRequirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from lightgbm) (0.24.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in c:\\users\\user\\.conda\\envs\\dm\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "##########Put your installation code here###############\n",
    "!pip install lightgbm\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test data prediction accuracy (LightGBM) : 0.977778\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "LGBM_model = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "LGBM_model = lgb.LGBMClassifier()\n",
    "LGBM_model.fit(X_train, Y_train)\n",
    "accuracy = LGBM_model.score(X_test, Y_test)   \n",
    "################################################# \n",
    "print('Test data prediction accuracy (LightGBM) : %f'%accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 5\n",
    "Naive Bayes classifier.\n",
    "- Scikit-learn의 Gaussian Naive Bayes classifier를 불러와 학습시키고, test데이터의 정확도를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test data prediction accuracy (NaiveBayes) : 0.844444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_model = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train, Y_train)\n",
    "accuracy = NB_model.score(X_test, Y_test)          \n",
    "################################################# \n",
    "print('Test data prediction accuracy (NaiveBayes) : %f'%accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 6\n",
    "SVM Bayes classifier.\n",
    "- Scikit-learn의 SVM classifier를 불러와 학습시키고, test데이터의 정확도를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test data prediction accuracy (SVC) : 0.980556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_model = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "SVM_model = SVC()\n",
    "SVM_model.fit(X_train, Y_train)\n",
    "accuracy = SVM_model.score(X_test, Y_test)    \n",
    "#################################################\n",
    "print('Test data prediction accuracy (SVC) : %f'%accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 7\n",
    "MLP classifier.\n",
    "- Scikit-learn의 MLP classifier를 불러와 학습시키고, test데이터의 정확도를 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test data prediction accuracy (MLPClassifier) : 0.961111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_model = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "MLP_model = MLPClassifier()\n",
    "MLP_model.fit(X_train, Y_train)\n",
    "accuracy = MLP_model.score(X_test, Y_test) \n",
    "#################################################\n",
    "print('Test data prediction accuracy (MLPClassifier) : %f'%accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# # Problem 8\n",
    "Ensemble classifier.\n",
    "- 위의 7개의 모델(CART, Random Forest, XGBoost, LightGBM, Naive Bayes, SVM, MLP)을 이용하여 Voting(7개의 예측 label중 가장 많이 예측된 label을 최종 prediction label로 택하는것)방법으로 최종 label을 택한 뒤, 예측 정확도를 출력하시오.\n",
    "- Scikit-learn의 VotingClassifier 패키지를 사용하여 작성.\n",
    "- 위에서 학습시킨 모델 변수명 사용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\DM\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[19:18:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test data prediction accuracy (Ensemble) : 0.975000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble_classifier = None\n",
    "accuracy = None\n",
    "############# Put your code here ################\n",
    "ensemble_classifier = VotingClassifier(\n",
    "    estimators=[('CART', CART_model), ('RF', Random_Forest_model), ('XGB', XGB_model),\n",
    "               ('LGBM', LGBM_model), ('NB', NB_model), ('SVM', SVM_model), ('MLP', MLP_model)],\n",
    "    voting='hard')\n",
    "ensemble_classifier.fit(X_train, Y_train)\n",
    "accuracy = ensemble_classifier.score(X_test, Y_test) \n",
    "################################################# \n",
    "print('Test data prediction accuracy (Ensemble) : %f'%accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0e7a462a002af671cc6048cea5a3f0c0c32843d4dfc10793721c7e21db3a95546",
   "display_name": "Python 3.7.10 64-bit ('DM': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}