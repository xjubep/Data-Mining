{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "colab": {
   "name": "DM_Assignment_4_120210183_강수연.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "f-sSFOFNsfmB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "#Cuda 혹은 cpu를 사용하시오.\n",
    "############Write Your Code Here############\n",
    "device = 'cuda'\n",
    "############################################\n",
    "\n",
    "\n",
    "#Custom_Dataset을 정의하시오.(10점)\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        #입력으로 들어온 X의 pixel값들을 0-1사이로 normalize하고 X의 shape을 (FB,C,H,W)로 변경하여 저장하여 self.X,self.y에 저장하시오.\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        ############Write Your Code Here############\n",
    "        self.X = (X / 255.0).reshape(-1, 3, 32, 32)\n",
    "        self.y = y\n",
    "        ############################################\n",
    "        \n",
    "    def __len__(self):\n",
    "        #Custom_Dataset에 저장되어있는 총 data의 개수를 result에 저장하여 반환하시오.\n",
    "        result = 0\n",
    "        ############Write Your Code Here############\n",
    "        result = len(self)\n",
    "        ############################################\n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #self.X, self.y 에서 idx에 맞는 data를 result_X,result_y에 저장하여 반환하시오.\n",
    "        result_X,result_y = None,None\n",
    "        ############Write Your Code Here############\n",
    "        result_X = self.X[idx]\n",
    "        result_y = self.y[idx]\n",
    "        ############################################\n",
    "        return result_X,result_y\n",
    "\n",
    "    \n",
    "#torch.nn을 사용하여 아래 함수들을 작성하시오. result는 nn.Layer중 하나이고 result를 반환함.(20점)\n",
    "def batch_norm(dim,for_MLP=True):\n",
    "    #for_MLP가 True일 시 MLP를 위한 BN Layer를 반환하고 False일 시 CNN을 위한 BN Layer를 반환함.\n",
    "    ############Write Your Code Here############\n",
    "    if for_MLP:\n",
    "        result = nn.BatchNorm1d(dim)\n",
    "    else:\n",
    "        result = nn.BatchNorm2d(dim)\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def fc_layer(in_dim,out_dim):\n",
    "    #Fully Connected Layer(Dense Layer)\n",
    "    ############Write Your Code Here############\n",
    "    result = nn.Linear(in_dim, out_dim)\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def conv_layer(in_ch,out_ch,kernel_size, stride=1, padding=0):\n",
    "    #Convolutional Layer for image\n",
    "    ############Write Your Code Here############\n",
    "    result = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding)\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def relu():\n",
    "    #ReLU function\n",
    "    ############Write Your Code Here############\n",
    "    result = nn.ReLU()\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def flatten():\n",
    "    #Flatten the data\n",
    "    ############Write Your Code Here############\n",
    "    result = nn.Flatten()\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "\n",
    "#skip_connection(bn -> relu -> conv -> bn -> relu -> conv)를 따르는 Res_block을 만드시오.\n",
    "#change_res가 True인 res_block을 통과한 feature map은 resolution이 2배 작아지고 channel의 깊이는 2배로 증가함. ex) 32*8*8 -> 64*4*4\n",
    "#위의 경우에는 skip_connection의 dimension은 1*1 conv로 맞춰줌.\n",
    "#change_res가 False인 Res_block을 통과한 feature map은 resolution과 channel의 깊이는 그대로 유지됨. ex) 32*4*4 -> 32*4*4(20점)\n",
    "class Res_block(nn.Module):\n",
    "    def __init__(self, input_channel, change_res):\n",
    "        super(Res_block,self).__init__()\n",
    "        self.change_res = change_res\n",
    "        if change_res:\n",
    "            ############Write Your Code Here############\n",
    "            self.conv_block2 = nn.Sequential(\n",
    "                batch_norm(input_channel),\n",
    "                relu(),\n",
    "                conv_layer(input_channel, 2*input_channel, 3, stride=2, padding=1)\n",
    "            )\n",
    "            self.skip_connection = conv_layer(input_channel, 2*input_channel, 1, stride=1, padding=0)\n",
    "            ############################################\n",
    "        else:\n",
    "            ############Write Your Code Here############\n",
    "            self.conv_block2 = nn.Sequential(\n",
    "                batch_norm(input_channel),\n",
    "                relu(),\n",
    "                conv_layer(input_channel, input_channel, 3, stride=1, padding=1)\n",
    "            )\n",
    "            self.skip_connection = None\n",
    "            ############################################\n",
    "        ############Write Your Code Here############\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            batch_norm(input_channel),\n",
    "            relu(),\n",
    "            conv_layer(input_channel, input_channel, 3, stride=1, padding=1)\n",
    "        )\n",
    "        ############################################\n",
    "    def forward(self,X):\n",
    "        ############Write Your Code Here############\n",
    "        Y = self.conv_block1(X)\n",
    "        Y = self.conv_block2(Y)\n",
    "        if change_res:\n",
    "            X = self.skip_connection(X)\n",
    "        X += Y\n",
    "        ############################################\n",
    "        return X\n",
    "\n",
    "    \n",
    "#Skip Connection을 이용하여 20개 이상의 layer를 가지고 테스트 셋에대하여 50%(70%) 이상의 성능을 주는 MLP를 만드시오.\n",
    "#nn.ModuleList를 사용하면 많을 층의 layer를 쌓는데 용이함.(20점), conv layer 사용 x\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP,self).__init__()\n",
    "        ############Write Your Code Here############\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            flatten(),\n",
    "            fc_layer(input_dim, 100),\n",
    "            relu(),\n",
    "        )\n",
    "        self.linears = nn.ModuleList([nn.Sequential(fc_layer(100, 100), relu()) for i in range(20)])\n",
    "        self.layer_n = nn.Sequential(\n",
    "            fc_layer(100, output_dim)\n",
    "        )\n",
    "        ############################################\n",
    "    def forward(self,X):\n",
    "        ############Write Your Code Here############\n",
    "        X = self.layer_1(X)\n",
    "        for layer in self.linears:\n",
    "            X = layer(X)\n",
    "        X = self.layer_n(X)\n",
    "        ############################################\n",
    "        return X\n",
    "        \n",
    "#Res_Block을 사용하여 테스트 셋에대한 75%(90%) 이상의 성능을 주는 CNN 모델을 만드시오.\n",
    "#flatten전에 nn.AdaptiveAvgPool2d를 사용하면 dimension맞추기가 쉬움.(20점)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channel, class_number, block_number):\n",
    "        super(CNN,self).__init__()\n",
    "        ############Write Your Code Here############\n",
    "        self.res1 = Res_block(3, False)\n",
    "        self.res2 = Res_block(3, True)\n",
    "        self.res3 = Res_block(6, False)        \n",
    "        self.res4 = Res_block(6, True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(16),\n",
    "            flatten(),\n",
    "            fc_layer(256, 512),\n",
    "            fc_layer(512, class_number)\n",
    "        )\n",
    "        ############################################\n",
    "    def forward(self,X):\n",
    "        ############Write Your Code Here############\n",
    "        X = self.res1(X)\n",
    "        X = self.res2(X)\n",
    "        X = self.res3(X)\n",
    "        X = self.res4(X)\n",
    "        X = self.fc(X)\n",
    "\n",
    "        ############################################\n",
    "        return X\n",
    "\n",
    "#loader에 있는 모든 data들에 대한 정확도를 구하여 accuracy에 저장하여 accuracy를 return하는 함수를 구현하시오.(10점)\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    total_example = 0\n",
    "    correct_example = 0\n",
    "    for data in loader:\n",
    "        x,y = data\n",
    "        x = torch.tensor(x, device = device)\n",
    "        y = torch.tensor(y, device = device)\n",
    "        ############Write Your Code Here############\n",
    "        outputs = model(x.float())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_example += (preds == y).sum()\n",
    "        total_example += preds.size(0)\n",
    "        ############################################\n",
    "    ############Write Your Code Here############\n",
    "    accuracy = float(correct_example) / total_example\n",
    "    ############################################\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "#epoch마다 train_loader에 있는 batch들을 사용하여 모델을 학습하고\n",
    "#epoch의 마지막 iteration에서는 모델의 validation accuracy를 확인하여 제일 높은 val. acc.를 가진 model을 best_model에 저장하고\n",
    "#val_acc에는 매 epoch마다 구해진 validation accuracy를 저장하시오.\n",
    "#running_loss에는 각각의 epoch에서 모든 batch의 loss를 다 더하여 저장하시오.\n",
    "#모든 epoch의 validation accuracy를 val_acc에 저장하여 best_model과 val_acc를 return하는 함수를 구현하시오.(10점)\n",
    "def train(model, epoches, train_loader, val_loader, optimizer, criteria):\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    batch_len = len(train_loader)\n",
    "    val_acc = []\n",
    "    for epoch in range(epoches):\n",
    "        running_loss = 0\n",
    "        for i,data in enumerate(train_loader):\n",
    "            x,y = data\n",
    "            x = torch.tensor(x, device = device)\n",
    "            y = torch.tensor(y, device = device)\n",
    "            ############Write Your Code Here############\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(x.float())\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criteria(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            ############################################\n",
    "            \n",
    "            #epoch의 마지막 iteration\n",
    "            if i % batch_len == batch_len-1:\n",
    "                print(f'{epoch+1}th iteration loss :',running_loss/batch_len)\n",
    "                running_loss = 0\n",
    "                ############Write Your Code Here############\n",
    "                epoch_acc = \n",
    "\n",
    "                val_acc.append(0)\n",
    "                if val_acc[epoch] > best_score:\n",
    "                    best_score = val_acc[epoch]\n",
    "                    best_model = copy.deepcopy(model.state_dict())\n",
    "                ############################################\n",
    "    return best_model, val_acc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ry8ERAt5sfmK",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "613f68cc-81bd-4053-a338-c793187f8d56",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#(50점)\n",
    "#Read the data\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True)\n",
    "testset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True)\n",
    "\n",
    "X_train, Y_train = trainset.data, np.array(trainset.targets)\n",
    "X_test, Y_test = testset.data, np.array(testset.targets)\n",
    "\n",
    "\n",
    "#앞서 정의한 Custom_Dataset과 DataLoader를 사용하여 train_loader,val_loader,test_loader를 정의하시오.\n",
    "#Batch_size는 본인의 컴퓨터 사향에 맞게 변경하면 됨. Validation Set으로 Train Set의 20%를 사용함.\n",
    "#Preprocessing\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "batch_size = 64\n",
    "############Write Your Code Here############\n",
    "train_loader = DataLoader(Custom_Dataset(X_train, Y_train), batch_size=batch_size, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(40000)), num_workers=4)\n",
    "val_loader = DataLoader(Custom_Dataset(X_train, Y_train), batch_size=batch_size,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(40000, 50000)), num_workers=4)\n",
    "test_loader = DataLoader(Custom_Dataset(X_test, Y_test), batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "############################################\n",
    "\n",
    "\n",
    "#앞서 정의한 MLP,CNN을 사용하여 mlp_model,cnn_model을 정의하시오.\n",
    "#Define the model\n",
    "mlp_model = None\n",
    "cnn_model = None\n",
    "############Write Your Code Here############\n",
    "mlp_model = MLP(3072, 10)\n",
    "cnn_model = CNN(3, 10, 4)\n",
    "############################################\n",
    "mlp_model.to(device)\n",
    "cnn_model.to(device)\n",
    "\n",
    "\n",
    "#앞서 정의한 train함수를 사용하여 best_mlp, mpl_val_acc, best_cnn, cnn_val_acc를 구하시오.\n",
    "#Train the model\n",
    "best_mlp = None\n",
    "mlp_val_acc = None\n",
    "best_cnn = None\n",
    "cnn_val_acc = None\n",
    "############Write Your Code Here############\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "bet_mlp, mlp_val_acc = train(mlp_model, 5, train_loader, val_loader, optimizer, criteria)\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "bet_cnn, cnn_val_acc = train(cnn_model, 5, train_loader, val_loader, optimizer, criteria)\n",
    "############################################\n",
    "\n",
    "\n",
    "#앞서 정의한 evaluate함수와 best_model들을 사용하여 mlp_acc, cnn_acc를 구하시오.\n",
    "#Test Accuracy\n",
    "mlp_acc = None  \n",
    "cnn_acc = None \n",
    "############Write Your Code Here############\n",
    "mlp_acc = evaluate(best_mlp, test_loader)\n",
    "cnn_acc = evaluate(best_cnn, test_loader)\n",
    "############################################\n",
    "print('MLP accuracy:',mlp_acc)\n",
    "print('CNN accuracy:',cnn_acc)\n",
    "\n",
    "\n",
    "#앞서 구한 val_acc들을 사용하여 이해 가능한 그래프를 그리시오.\n",
    "#Validation Accuracy Plot\n",
    "############Write Your Code Here############\n",
    "plt.plot(mlp_val_acc)\n",
    "plt.plot(cnn_val_acc)\n",
    "############################################"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "1th iteration loss : 2.3031842697143556\n",
      "2th iteration loss : 2.3030340682983397\n",
      "3th iteration loss : 2.3028968070983886\n",
      "4th iteration loss : 2.302853929901123\n",
      "5th iteration loss : 2.3027852333068846\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-c41df16f87c1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0mbet_mlp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmlp_val_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmlp_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriteria\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[0moptimizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcnn_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.001\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m \u001B[0mbet_cnn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcnn_val_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcnn_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriteria\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m \u001B[0;31m############################################\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-828a180e1f7c>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, epoches, train_loader, val_loader, optimizer, criteria)\u001B[0m\n\u001B[1;32m    221\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 223\u001B[0;31m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    224\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriteria\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-828a180e1f7c>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m         \u001B[0;31m############Write Your Code Here############\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mres1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    174\u001B[0m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mres2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mres3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-828a180e1f7c>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m         \u001B[0;31m############Write Your Code Here############\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m         \u001B[0mY\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_block1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m         \u001B[0mY\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_block2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mY\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mchange_res\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 119\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    120\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 100\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_input_dim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m         \u001B[0;31m# exponential_average_factor is set to self.momentum\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001B[0m in \u001B[0;36m_check_input_dim\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    211\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m2\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    212\u001B[0m             raise ValueError('expected 2D or 3D input (got {}D input)'\n\u001B[0;32m--> 213\u001B[0;31m                              .format(input.dim()))\n\u001B[0m\u001B[1;32m    214\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: expected 2D or 3D input (got 4D input)"
     ]
    }
   ]
  }
 ]
}